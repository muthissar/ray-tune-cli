reporter:
  metric_columns:
    - loss
    - mean_accuracy
    - training_iteration
  parameter_columns:
    - model.layer_1_size
    - model.layer_2_size
    - model.lr
    - model.batch_size

tune_callback:
  class_path: TuneReportCallback
  init_args:
    metrics:
      loss: ptl/val_loss
      mean_accuracy: ptl/val_accuracy
    'on': validation_end
  #class_path: TuneReportCheckpointCallback
  #init_args:
  #  metrics:
  #    loss: ptl/val_loss
  #    mean_accuracy: ptl/val_accuracy
  #  filename: checkpoint
  #  'on': validation_end
run:
  metric: loss
  mode: min
  num_samples: 10
  resources_per_trial:
    cpu: 8
    gpu: 1

  name: tune_mnist_asha
  scheduler:
    class_path: AsyncHyperBandScheduler
    init_args:
      max_t: 10
      grace_period: 1
      reduction_factor: 2
  callbacks:
    - class_path: ray.air.integrations.wandb.WandbLoggerCallback
      init_args:
        project: nmist_test
        # save_dir: out/nmist_test/lightning_logs
        # log_config: true
      dict_kwargs:
        group: sweep

  #name: tune_mnist_pbt
  #scheduler:
  #  class_path: PopulationBasedTraining
  #  init_args:
  #    perturbation_interval: 4
  #    hyperparam_mutations:
  #      model.lr: tune.loguniform(1e-4, 1e-1)
  #      model.batch_size: '[32, 64, 128]'

  config:
    model.layer_1_size: tune.choice([16, 32, 64])
    model.layer_2_size: tune.choice([16, 32, 64])
    model.lr: tune.loguniform(1e-4, 1e-1)
    model.batch_size: tune.choice([16, 32, 64])
